{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32cf8f5c-a370-402c-a1af-76a707334547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import json\n",
    "import onnx\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Visualize the embeddings using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Specify the path to the kaleido runtime executable\n",
    "# Replace '/path/to/kaleido' with the actual path to the kaleido executable\n",
    "# pio.kaleido.scope.path = 'C:\\1\\a\\Lib\\site-packages\\kaleido\\executable\\bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e692a5e-1fdc-4b16-b6d9-426e3724acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca2826f3-082a-465f-9368-a8df8b2721a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model and tokenizer\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# model = BertModel.from_pretrained(model_name, output_attentions=True)\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b655fa7-a2f3-4229-8cb5-f9954b0ec4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the model is on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "o = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b97467-d904-4690-bb55-d3a466a8ebff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b8692e-dbcf-497d-b69f-b62e124ac6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BertModel(\\n  (embeddings): BertEmbeddings(\\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\\n    (position_embeddings): Embedding(512, 768)\\n    (token_type_embeddings): Embedding(2, 768)\\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n    (dropout): Dropout(p=0.1, inplace=False)\\n  )\\n  (encoder): BertEncoder(\\n    (layer): ModuleList(\\n      (0-11): 12 x BertLayer(\\n        (attention): BertAttention(\\n          (self): BertSdpaSelfAttention(\\n            (query): Linear(in_features=768, out_features=768, bias=True)\\n            (key): Linear(in_features=768, out_features=768, bias=True)\\n            (value): Linear(in_features=768, out_features=768, bias=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n          (output): BertSelfOutput(\\n            (dense): Linear(in_features=768, out_features=768, bias=True)\\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n            (dropout): Dropout(p=0.1, inplace=False)\\n          )\\n        )\\n        (intermediate): BertIntermediate(\\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\\n          (intermediate_act_fn): GELUActivation()\\n        )\\n        (output): BertOutput(\\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n          (dropout): Dropout(p=0.1, inplace=False)\\n        )\\n      )\\n    )\\n  )\\n  (pooler): BertPooler(\\n    (dense): Linear(in_features=768, out_features=768, bias=True)\\n    (activation): Tanh()\\n  )\\n)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d96aef-95e4-44fe-9474-1315e28e3e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('embeddings',\n",
       "               BertEmbeddings(\n",
       "                 (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "                 (position_embeddings): Embedding(512, 768)\n",
       "                 (token_type_embeddings): Embedding(2, 768)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )),\n",
       "              ('encoder',\n",
       "               BertEncoder(\n",
       "                 (layer): ModuleList(\n",
       "                   (0-11): 12 x BertLayer(\n",
       "                     (attention): BertAttention(\n",
       "                       (self): BertSdpaSelfAttention(\n",
       "                         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                       (output): BertSelfOutput(\n",
       "                         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                         (dropout): Dropout(p=0.1, inplace=False)\n",
       "                       )\n",
       "                     )\n",
       "                     (intermediate): BertIntermediate(\n",
       "                       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (intermediate_act_fn): GELUActivation()\n",
       "                     )\n",
       "                     (output): BertOutput(\n",
       "                       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )),\n",
       "              ('pooler',\n",
       "               BertPooler(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (activation): Tanh()\n",
       "               ))]),\n",
       " 'config': BertConfig {\n",
       "   \"_name_or_path\": \"bert-base-uncased\",\n",
       "   \"architectures\": [\n",
       "     \"BertForMaskedLM\"\n",
       "   ],\n",
       "   \"attention_probs_dropout_prob\": 0.1,\n",
       "   \"classifier_dropout\": null,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"hidden_act\": \"gelu\",\n",
       "   \"hidden_dropout_prob\": 0.1,\n",
       "   \"hidden_size\": 768,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 3072,\n",
       "   \"layer_norm_eps\": 1e-12,\n",
       "   \"max_position_embeddings\": 512,\n",
       "   \"model_type\": \"bert\",\n",
       "   \"num_attention_heads\": 12,\n",
       "   \"num_hidden_layers\": 12,\n",
       "   \"output_attentions\": true,\n",
       "   \"pad_token_id\": 0,\n",
       "   \"position_embedding_type\": \"absolute\",\n",
       "   \"transformers_version\": \"4.44.2\",\n",
       "   \"type_vocab_size\": 2,\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 30522\n",
       " },\n",
       " 'name_or_path': 'bert-base-uncased',\n",
       " 'warnings_issued': {},\n",
       " 'generation_config': None,\n",
       " '_keep_in_fp32_modules': None,\n",
       " 'attn_implementation': 'sdpa',\n",
       " 'position_embedding_type': 'absolute',\n",
       " '_is_hf_initialized': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aacaa415-ccc4-4b11-84ff-302890ab9bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_assisted_decoding',\n",
       " '_auto_class',\n",
       " '_autoset_attn_implementation',\n",
       " '_backward_compatibility_gradient_checkpointing',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_beam_search',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_check_and_enable_flash_attn_2',\n",
       " '_check_and_enable_sdpa',\n",
       " '_compiled_call_impl',\n",
       " '_constrained_beam_search',\n",
       " '_contrastive_search',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_copy_lm_head_original_to_resized',\n",
       " '_create_repo',\n",
       " '_dispatch_accelerate_model',\n",
       " '_dola_decoding',\n",
       " '_expand_inputs_for_generation',\n",
       " '_extract_past_from_model_output',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_from_config',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_cache',\n",
       " '_get_candidate_generator',\n",
       " '_get_files_timestamps',\n",
       " '_get_initial_cache_position',\n",
       " '_get_logits_processor',\n",
       " '_get_logits_warper',\n",
       " '_get_name',\n",
       " '_get_no_split_modules',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_get_stopping_criteria',\n",
       " '_group_beam_search',\n",
       " '_has_unfinished_sequences',\n",
       " '_hf_peft_config_loaded',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_weights',\n",
       " '_initialize_weights',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_is_quantized_training_enabled',\n",
       " '_is_stateful',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_state_dict',\n",
       " '_load_pretrained_model',\n",
       " '_load_pretrained_model_low_mem',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_initialize_input_ids_for_generation',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_merge_criteria_processor_list',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_no_split_modules',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_generated_length',\n",
       " '_prepare_generation_config',\n",
       " '_prepare_model_inputs',\n",
       " '_prepare_special_tokens',\n",
       " '_prune_heads',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_sample',\n",
       " '_save_to_state_dict',\n",
       " '_set_default_torch_dtype',\n",
       " '_set_gradient_checkpointing',\n",
       " '_skip_keys_device_placement',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_supports_cache_class',\n",
       " '_supports_default_dynamic_cache',\n",
       " '_supports_flash_attn_2',\n",
       " '_supports_quantized_cache',\n",
       " '_supports_sdpa',\n",
       " '_supports_static_cache',\n",
       " '_temporary_reorder_cache',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_tied_weights_keys',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_upload_modified_files',\n",
       " '_validate_assistant',\n",
       " '_validate_generated_length',\n",
       " '_validate_model_class',\n",
       " '_validate_model_kwargs',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'active_adapter',\n",
       " 'active_adapters',\n",
       " 'add_adapter',\n",
       " 'add_memory_hooks',\n",
       " 'add_model_tags',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'attn_implementation',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'can_generate',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'compute_transition_scores',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'contrastive_search',\n",
       " 'cpu',\n",
       " 'create_extended_attention_mask_for_decoder',\n",
       " 'cuda',\n",
       " 'dequantize',\n",
       " 'device',\n",
       " 'disable_adapters',\n",
       " 'disable_input_require_grads',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'embeddings',\n",
       " 'enable_adapters',\n",
       " 'enable_input_require_grads',\n",
       " 'encoder',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'generation_config',\n",
       " 'get_adapter_state_dict',\n",
       " 'get_buffer',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_extra_state',\n",
       " 'get_head_mask',\n",
       " 'get_input_embeddings',\n",
       " 'get_memory_footprint',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameter',\n",
       " 'get_position_embeddings',\n",
       " 'get_submodule',\n",
       " 'gradient_checkpointing_disable',\n",
       " 'gradient_checkpointing_enable',\n",
       " 'half',\n",
       " 'heal_tokens',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'ipu',\n",
       " 'is_gradient_checkpointing',\n",
       " 'is_parallelizable',\n",
       " 'load_adapter',\n",
       " 'load_state_dict',\n",
       " 'load_tf_weights',\n",
       " 'main_input_name',\n",
       " 'model_tags',\n",
       " 'modules',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_parameters',\n",
       " 'parameters',\n",
       " 'pooler',\n",
       " 'position_embedding_type',\n",
       " 'post_init',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'push_to_hub',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_for_auto_class',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_position_embeddings',\n",
       " 'resize_token_embeddings',\n",
       " 'retrieve_modules_from_names',\n",
       " 'reverse_bettertransformer',\n",
       " 'save_pretrained',\n",
       " 'set_adapter',\n",
       " 'set_extra_state',\n",
       " 'set_input_embeddings',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'supports_gradient_checkpointing',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'to_bettertransformer',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'warn_if_padding_and_no_attention_mask',\n",
       " 'warnings_issued',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3843d43d-fb74-4f10-a5a2-6ab016461f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's dictionary\n",
    "model_dict = model.__dict__\n",
    "model_str = model.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3c63c3-0198-40b3-a26a-1dd4e57dcbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's architecture\n",
    "model_config = model.config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e53241b-1117-4e55-8456-d4ca781fa768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_json_serializable(obj):\n",
    "    if isinstance(obj, (int, float, bool, str, type(None))):\n",
    "        return obj\n",
    "    elif isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_json_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_to_json_serializable(item) for item in obj]\n",
    "    else:\n",
    "        return str(obj)  # Fallback to string representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3da3e18d-0a11-4470-b6af-5d8d388247ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the model's dictionary to a JSON-compatible format\n",
    "json_serializable_model_dict = convert_to_json_serializable(model_dict)\n",
    "json_serializable_model_str = convert_to_json_serializable(model_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2517bc8-db45-4ed5-ba3e-5201c40166ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model's architecture to a JSON file\n",
    "with open('./model_architecture.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a676a13-9fdf-4277-8714-205de18dd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model's dictionary to a JSON file\n",
    "with open('./model_dict.json', 'w') as f:\n",
    "    json.dump(json_serializable_model_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdf4dfa2-201f-478c-ba4f-d356bb7cd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model's dictionary to a JSON file\n",
    "# with open('./model_str.json', 'w') as f:\n",
    "#     json.dump(model_str, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44030e38-4df3-404a-bc53-7e30c63ab3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "embeddings = model.get_input_embeddings()\n",
    "torch.save(state_dict, './model_state_dict')\n",
    "torch.save(embeddings.state_dict(), './model_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27d71b75-0fc3-4c2c-ac00-69ba4a8d3e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_fill_padding_idx_with_zero',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'embedding_dim',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'max_norm',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'norm_type',\n",
       " 'num_embeddings',\n",
       " 'padding_idx',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'scale_grad_by_freq',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'sparse',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1d97fc9-6bc4-4ef5-84d3-f83bd412708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_and_types(state_dict, parent_key='', summaries=None):\n",
    "    if summaries is None:\n",
    "        summaries = []\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = f\"{parent_key}.{key}\" if parent_key else key\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            summaries.append((new_key, str(value.dtype)))\n",
    "        elif isinstance(value, dict):\n",
    "            get_keys_and_types(value, new_key, summaries)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d3023ce-9ab9-4607-b621-f61cb02b9bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keys and data types\n",
    "keys_and_types = get_keys_and_types(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cac88da4-71d1-44bb-98b4-0cd3d4ef6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the keys and data types to a JSON file\n",
    "with open('./model_structure.json', 'w') as f:\n",
    "    json.dump(keys_and_types, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f172a1bb-3051-4f7e-ae16-a90ca1568c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    \"embeddings.word_embeddings.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"embeddings.position_embeddings.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"embeddings.token_type_embeddings.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"embeddings.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"embeddings.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.attention.self.query.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.attention.self.query.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.attention.self.key.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.attention.self.key.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.attention.self.value.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.attention.self.value.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.attention.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.attention.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.attention.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.attention.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.intermediate.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.intermediate.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.0.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.attention.self.query.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.attention.self.query.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.attention.self.key.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.attention.self.key.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.attention.self.value.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.attention.self.value.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.attention.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.attention.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.attention.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.attention.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.intermediate.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.intermediate.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.1.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.attention.self.query.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.attention.self.query.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.attention.self.key.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.attention.self.key.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.attention.self.value.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.attention.self.value.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.attention.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.attention.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.attention.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.attention.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.intermediate.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.intermediate.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.2.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.attention.self.query.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.attention.self.query.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.attention.self.key.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.attention.self.key.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.attention.self.value.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.attention.self.value.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.attention.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.attention.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.attention.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.attention.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.intermediate.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.intermediate.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.3.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.attention.self.query.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.attention.self.query.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.attention.self.key.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.attention.self.key.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.attention.self.value.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.attention.self.value.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.attention.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.attention.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.attention.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.attention.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.intermediate.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.intermediate.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.4.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.attention.self.query.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.attention.self.query.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.attention.self.key.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.attention.self.key.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.attention.self.value.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.attention.self.value.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.attention.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.attention.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.attention.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.attention.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.intermediate.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.intermediate.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.5.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.attention.self.query.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.attention.self.query.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.attention.self.key.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.attention.self.key.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.attention.self.value.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.attention.self.value.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.attention.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.attention.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.attention.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.attention.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.intermediate.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.intermediate.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.6.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.attention.self.query.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.attention.self.query.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.attention.self.key.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.attention.self.key.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.attention.self.value.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.attention.self.value.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.attention.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.attention.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.attention.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.attention.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.intermediate.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.intermediate.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.7.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.attention.self.query.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.attention.self.query.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.attention.self.key.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.attention.self.key.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.attention.self.value.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.attention.self.value.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.attention.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.attention.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.attention.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.attention.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.intermediate.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.intermediate.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.8.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.attention.self.query.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.attention.self.query.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.attention.self.key.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.attention.self.key.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.attention.self.value.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.attention.self.value.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.attention.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.attention.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.attention.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.attention.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.intermediate.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.intermediate.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.9.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.attention.self.query.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.attention.self.query.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.attention.self.key.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.attention.self.key.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.attention.self.value.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.attention.self.value.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.attention.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.attention.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.attention.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.attention.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.intermediate.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.intermediate.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.10.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.attention.self.query.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.attention.self.query.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.attention.self.key.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.attention.self.key.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.attention.self.value.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.attention.self.value.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.attention.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.attention.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.attention.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.attention.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.intermediate.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.intermediate.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.output.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.output.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.output.LayerNorm.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"encoder.layer.11.output.LayerNorm.bias\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"pooler.dense.weight\",\n",
      "    \"torch.float32\"\n",
      "  ],\n",
      "  [\n",
      "    \"pooler.dense.bias\",\n",
      "    \"torch.float32\"\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Print the keys and data types\n",
    "print(json.dumps(keys_and_types, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c38cf8d-4ff6-4867-b0b2-2c5d8516b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_modules(ordered_dict, indent=0):\n",
    "    lines = []\n",
    "    for key, value in ordered_dict.items():\n",
    "        if isinstance(value, torch.nn.Module):\n",
    "            lines.append(' ' * indent + f\"{key}: {type(value).__name__} ({len(value._modules)} sub-modules)\")\n",
    "            lines.extend(format_modules(value._modules, indent=indent + 2))\n",
    "        else:\n",
    "            lines.append(' ' * indent + f\"{key}: {type(value).__name__}\")\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63f7cceb-43ea-4cc0-9639-c895a068e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_formatted_string = '\\n'.join(format_modules(model._modules))\n",
    "# Save the model's dictionary to a JSON file\n",
    "with open('./model_str2.txt', 'w') as f:\n",
    "    for line in model._modules.__repr__():\n",
    "        f.write(line)\n",
    "f.close()\n",
    "    # json.dump(model_formatted_string, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe0310df-8bce-4c0c-8ae8-bc2e626587aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LayerNorm',\n",
       " 'T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dropout',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'position_embedding_type',\n",
       " 'position_embeddings',\n",
       " 'position_ids',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'token_type_embeddings',\n",
       " 'token_type_ids',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'word_embeddings',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ab63899-0263-4af3-ad0b-b285c6e567d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word embeddings\n",
    "word_embeddings = model.embeddings.word_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50911304-9ee0-45fe-832a-86e79145b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the embeddings to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "word_embeddings = word_embeddings.to(device)\n",
    "\n",
    "# Get the tokens and their corresponding indices\n",
    "vocab = tokenizer.get_vocab()\n",
    "tokens = [tokenizer.convert_ids_to_tokens(i) for i in range(len(vocab))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3a016ea-39d9-489f-8384-e8d1b4c729bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "word_embeddings_2d = pca.fit_transform(word_embeddings.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "401e9d0c-9b60-4623-a2fc-d206ad4b0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(word_embeddings_2d, columns=['Semantic Component', 'Syntactic Component'])\n",
    "df['Token'] = tokens\n",
    "\n",
    "# Define a color based on token length\n",
    "df['Color'] = df['Token'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172eaa0-57de-4458-9cab-dece1dadb122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the embeddings using Plotly\n",
    "fig = px.scatter(df, x='Semantic Component', y='Syntactic Component', color='Color', opacity=0.15)\n",
    "fig.update_traces(marker=dict(size=5, line=dict(width=0.5, color='DarkSlateGrey')), selector=dict(mode='markers'))\n",
    "fig.update_layout(title='Word Embeddings Visualization', xaxis_title='Semantic Component', yaxis_title='Syntactic Component', showlegend=False, width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c4b026e-2e26-43db-b293-3acaf46554f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5387caf3-3cba-4f11-9057-b005fa76faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word embeddings in the model\n",
    "word_embeddings = model.embeddings.word_embeddings.weight.data.cpu().numpy()\n",
    "\n",
    "# Perform PCA on the word embeddings\n",
    "pca = PCA(n_components=2)\n",
    "word_embeddings_2d = pca.fit_transform(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11b41a-7573-462d-8b9f-1aaad41011c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to visualize the word embeddings\n",
    "fig = go.Figure(data=go.Scattergl(\n",
    "    x=word_embeddings_2d[:, 0],  # Use the first PCA component as x-axis\n",
    "    y=word_embeddings_2d[:, 1],  # Use the second PCA component as y-axis\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color='gray',  # Default color for embeddings\n",
    "        size=5  # Default size for embeddings\n",
    "    ),\n",
    "    opacity=0.07,\n",
    "    text=[tokenizer.convert_ids_to_tokens(i) for i in range(tokenizer.vocab_size)],  # Use the tokens as text labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d20fdc-8da1-41ba-8249-953f0bb9420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = \"mango meade\"\n",
    "prompt_tokens = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n",
    "prompt_tokens = {key: value.to(device) for key, value in prompt_tokens.items()}\n",
    "\n",
    "# Get the attention weights for the prompt\n",
    "with torch.no_grad():\n",
    "    prompt_outputs = model(**prompt_tokens)\n",
    "    attentions = prompt_outputs['attentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf2c5e4-5db8-40b4-ade3-f69239c676ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate and normalize attention weights across all layers and heads\n",
    "aggregated_attentions = torch.stack(attentions).sum(dim=0).squeeze(0).cpu().numpy()\n",
    "normalized_attentions = (aggregated_attentions - aggregated_attentions.min()) / (aggregated_attentions.max() - aggregated_attentions.min())\n",
    "\n",
    "# Sum attention scores across all layers and heads for each token\n",
    "token_attention_scores = np.sum(normalized_attentions, axis=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd58553-8609-4cb2-b0e9-a89aa4e90cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the word embeddings that pass through the attention mechanism\n",
    "highlighted_tokens = set()\n",
    "for i, attention_score in enumerate(token_attention_scores):\n",
    "    if attention_score > 0:  # Check if the attention score for this token is greater than 0\n",
    "        highlighted_tokens.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b7868-6813-4155-82b7-23cdda5e3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update marker color and size for highlighted word embeddings\n",
    "for i in highlighted_tokens:\n",
    "    fig.add_trace(go.Scattergl(\n",
    "        x=[word_embeddings_2d[i, 0]],\n",
    "        y=[word_embeddings_2d[i, 1]],\n",
    "        mode='markers',\n",
    "        marker=dict(color='red', size=10),\n",
    "        text=tokenizer.convert_ids_to_tokens(i)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b9caee-7213-4129-9e43-7ec465698b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update layout and show plot\n",
    "fig.update_layout(title='BERT Word Embeddings Visualization with Attention Highlight', xaxis_title='PCA Component 1', yaxis_title='PCA Component 2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b46af-efa2-4f12-bc30-8a84c8b1f7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
