{
    "training": false,
    "_parameters": {},
    "_buffers": {},
    "_non_persistent_buffers_set": [],
    "_backward_pre_hooks": {},
    "_backward_hooks": {},
    "_is_full_backward_hook": null,
    "_forward_hooks": {},
    "_forward_hooks_with_kwargs": {},
    "_forward_hooks_always_called": {},
    "_forward_pre_hooks": {},
    "_forward_pre_hooks_with_kwargs": {},
    "_state_dict_hooks": {},
    "_state_dict_pre_hooks": {},
    "_load_state_dict_pre_hooks": {},
    "_load_state_dict_post_hooks": {},
    "_modules": {
        "embeddings": "BertEmbeddings(\n  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n  (position_embeddings): Embedding(512, 768)\n  (token_type_embeddings): Embedding(2, 768)\n  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)",
        "encoder": "BertEncoder(\n  (layer): ModuleList(\n    (0-11): 12 x BertLayer(\n      (attention): BertAttention(\n        (self): BertSelfAttention(\n          (query): Linear(in_features=768, out_features=768, bias=True)\n          (key): Linear(in_features=768, out_features=768, bias=True)\n          (value): Linear(in_features=768, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output): BertSelfOutput(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (intermediate): BertIntermediate(\n        (dense): Linear(in_features=768, out_features=3072, bias=True)\n        (intermediate_act_fn): GELUActivation()\n      )\n      (output): BertOutput(\n        (dense): Linear(in_features=3072, out_features=768, bias=True)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n)",
        "pooler": "BertPooler(\n  (dense): Linear(in_features=768, out_features=768, bias=True)\n  (activation): Tanh()\n)"
    },
    "config": "BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_attentions\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.32.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n",
    "name_or_path": "bert-base-uncased",
    "warnings_issued": {},
    "generation_config": null,
    "_is_hf_initialized": true,
    "is_loaded_in_4bit": false,
    "is_loaded_in_8bit": false
}